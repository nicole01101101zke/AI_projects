{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "seq2seq_en_de_10000.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXbqdGQOHibx",
        "outputId": "5c2fe8a6-f8f3-4602-e51b-e14d9ca7977f"
      },
      "source": [
        "!pip install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.10.0.2)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed torchtext-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVQUx_3UU6IL",
        "outputId": "f5bb5bf8-9a1f-44ac-8024-1958023c9e38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQlOGDqy0vRo",
        "outputId": "76aea81e-bd5f-4dfd-8201-3217db43066d"
      },
      "source": [
        "!pip install '/content/drive/MyDrive/AI/project3/dataset/en_core_web_md-3.2.0.tar.gz'\n",
        "!pip install '/content/drive/MyDrive/AI/project3/dataset/de_core_news_md-3.2.0.tar.gz'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./drive/MyDrive/AI/project3/dataset/en_core_web_md-3.2.0.tar.gz\n",
            "Collecting spacy<3.3.0,>=3.2.0\n",
            "  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-3.2.0-py3-none-any.whl size=45684465 sha256=ca12fb9a19fb811c9eafddf10a22662333084f7c1bfd8b43dc9422110efa8acb\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/79/d2/b7ad871b150b1942ca4fe921c603e71c225e780276a69c6331\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, en-core-web-md\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 en-core-web-md-3.2.0 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.0 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Processing ./drive/MyDrive/AI/project3/dataset/de_core_news_md-3.2.0.tar.gz\n",
            "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-md==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (0.8.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-md==3.2.0) (2.0.1)\n",
            "Building wheels for collected packages: de-core-news-md\n",
            "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-md: filename=de_core_news_md-3.2.0-py3-none-any.whl size=48819390 sha256=0c280d3ee697f7f69cca49e1db8ef3be301924683ec15777f58628f8c5b1cae4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/e1/20/7fc8144387c55591954ea279c0da7be6839ce3cde7214a5c02\n",
            "Successfully built de-core-news-md\n",
            "Installing collected packages: de-core-news-md\n",
            "Successfully installed de-core-news-md-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFSAJnIcTUKj",
        "outputId": "61f458a7-0176-4563-e7b7-72ef56c58601"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import io\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, FastText\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch import Tensor\n",
        "from torch import zeros\n",
        "from typing import Tuple\n",
        "\n",
        "random.seed(20)\n",
        "torch.manual_seed(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fef88da5150>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzN2wvhLuT7N"
      },
      "source": [
        "nlp1 = spacy.load('en_core_web_md')\n",
        "nlp2 = spacy.load('de_core_news_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZoJjmwTYd7O"
      },
      "source": [
        "测试集预处理，将.sgm格式的文件中的句子内容抽取出来，存入.txt文件"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3i0vSaWVbu1"
      },
      "source": [
        "eng_test = []\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/newstest2016-ende-src.en.sgm\", \"r\") as f1:\n",
        "  data3 = f1.read().split(\"\\n\")\n",
        "  for i in range(len(data3)):\n",
        "    temp = re.findall(r'<seg.*?>(.*?)</seg>',data3[i],re.S|re.M)\n",
        "    if not temp:\n",
        "      continue\n",
        "    else:\n",
        "      eng_test.append(temp[0])\n",
        "f1.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/en_test.txt\", \"w\") as f2:\n",
        "  for line in eng_test:\n",
        "    f2.write(line+\"\\n\")\n",
        "f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VySCQzqRYsA6"
      },
      "source": [
        "de_test = []\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/newstest2016-ende-ref.de.sgm\", \"r\") as f3:\n",
        "  data4 = f3.read().split(\"\\n\")\n",
        "  for i in range(len(data4)):\n",
        "    temp = re.findall(r'<seg.*?>(.*?)</seg>',data4[i],re.S|re.M)\n",
        "    if not temp:\n",
        "      continue\n",
        "    else:\n",
        "      de_test.append(temp[0])\n",
        "f3.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/de_test.txt\", \"w\") as f4:\n",
        "  for line in de_test:\n",
        "    f4.write(line+\"\\n\")\n",
        "f4.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AwqZ2xIY7bN"
      },
      "source": [
        "en_train = []\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/news-commentary-v11.de-en.en\", \"r\") as f5:\n",
        "  data4 = f5.read().split(\"\\n\")\n",
        "  en_train = data4[0:10000]\n",
        "\n",
        "f5.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/en_train.txt\", \"w\") as f6:\n",
        "  for line in en_train:\n",
        "    f6.write(line+\"\\n\")\n",
        "f6.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nah4uHh0ZswA"
      },
      "source": [
        "de_train = []\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/news-commentary-v11.de-en.de\", \"r\") as f7:\n",
        "  data4 = f7.read().split(\"\\n\")\n",
        "  de_train = data4[0:10000]\n",
        "\n",
        "f7.close()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/AI/project3/dataset/de_train.txt\", \"w\") as f8:\n",
        "  for line in de_train:\n",
        "    f8.write(line+\"\\n\")\n",
        "f8.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn8BpVjOTzgb"
      },
      "source": [
        "class DataPipeline:\n",
        "\n",
        "    def __init__(self, batch_size=64):\n",
        "        train_datasets = [\"/content/drive/MyDrive/AI/project3/dataset/en_train.txt\", \"/content/drive/MyDrive/AI/project3/dataset/de_train.txt\"]\n",
        "        val_datasets = [\"/content/drive/MyDrive/AI/project3/dataset/newstest2010.en\", \"/content/drive/MyDrive/AI/project3/dataset/newstest2010.de\"]\n",
        "        test_datasets = [\"/content/drive/MyDrive/AI/project3/dataset/en_test.txt\", \"/content/drive/MyDrive/AI/project3/dataset/de_test.txt\"]\n",
        "        self.en_tokenizer = get_tokenizer('spacy', language='en_core_web_md')\n",
        "        self.de_tokenizer = get_tokenizer('spacy', language='de_core_news_md')\n",
        "        self.en_vocab = self.build_en_vocab(train_datasets[0], self.en_tokenizer)\n",
        "        self.de_vocab = self.build_de_vocab(train_datasets[1], self.de_tokenizer)\n",
        "\n",
        "        train_data = self.create_tensor(train_datasets)\n",
        "        val_data = self.create_tensor(val_datasets)\n",
        "        test_data = self.create_tensor(test_datasets)\n",
        "\n",
        "        self.train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=self.create_batch)\n",
        "        self.valid_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=self.create_batch)\n",
        "        self.test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=self.create_batch)\n",
        "\n",
        "    def build_en_vocab(self, filepath, tokenizer):\n",
        "        counter = Counter()\n",
        "        with io.open(filepath, encoding=\"utf8\") as f1:\n",
        "            for temp in f1:\n",
        "                counter.update(tokenizer(temp))\n",
        "        vocab = Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'], vectors=FastText(language='en', max_vectors=1000_000))\n",
        "        zero_vec = torch.zeros(vocab.vectors.size()[0])\n",
        "        zero_vec = torch.unsqueeze(zero_vec, dim=1)\n",
        "        vocab.vectors = torch.cat((zero_vec, zero_vec, zero_vec, vocab.vectors), dim=1)\n",
        "        vocab.vectors[vocab[\"<pad>\"]][0] = 1\n",
        "        vocab.vectors[vocab[\"<bos>\"]][1] = 1\n",
        "        vocab.vectors[vocab[\"<eos>\"]][2] = 1\n",
        "        return vocab\n",
        "\n",
        "    def build_de_vocab(self, filepath, tokenizer):\n",
        "        counter = Counter()\n",
        "        with io.open(filepath, encoding=\"utf8\") as f:\n",
        "            for temp in f:\n",
        "                counter.update(tokenizer(temp))\n",
        "        return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "    def create_batch(self, data_batch):\n",
        "        en_batch, de_batch, = [], []\n",
        "        for (en_item, de_item) in data_batch:\n",
        "            en_batch.append(torch.cat([torch.tensor([self.en_vocab['<bos>']]), en_item, torch.tensor([self.en_vocab['<eos>']])], dim=0))\n",
        "            de_batch.append(torch.cat([torch.tensor([self.de_vocab['<bos>']]), de_item, torch.tensor([self.de_vocab['<eos>']])], dim=0))\n",
        "        en_batch = pad_sequence(en_batch, padding_value=self.en_vocab['<pad>'])\n",
        "        de_batch = pad_sequence(de_batch, padding_value=self.de_vocab['<pad>'])\n",
        "        return en_batch, de_batch\n",
        "\n",
        "    def create_tensor(self, filepaths):\n",
        "        raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "        raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "        data = []\n",
        "        #cnt = 0\n",
        "        for (raw_en, raw_de) in zip(raw_en_iter, raw_de_iter):\n",
        "            #cnt += 1\n",
        "            en_tensor_ = torch.tensor([self.en_vocab[token] for token in self.en_tokenizer(raw_en)[:-1]], dtype=torch.long)  \n",
        "            de_tensor_ = torch.tensor([self.de_vocab[token] for token in self.de_tokenizer(raw_de)[:-1]], dtype=torch.long)  \n",
        "            data.append((en_tensor_, de_tensor_))\n",
        "            #if cnt >= 20000:\n",
        "              #break\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUA0f1VcbDL"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim_input: int, dim_en_embedding: int, dim_en_hidden: int, dim_de_hidden: int, dropout: float):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_en_embedding = dim_en_embedding\n",
        "        self.dim_en_hidden = dim_en_hidden\n",
        "        self.dim_de_hidden = dim_de_hidden\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(dim_input, dim_en_embedding).from_pretrained(en_vocab.vectors, freeze=True)\n",
        "        self.rnn = nn.GRU(dim_en_embedding, dim_en_hidden, num_layers=1, bidirectional=True)\n",
        "        self.fc = nn.Linear(dim_en_hidden * 2, dim_de_hidden)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))\n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh5ZkKgQdB5F"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim_en_hidden: int, dim_de_hidden: int, dim_attention: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_en_hidden = dim_en_hidden\n",
        "        self.dim_de_hidden = dim_de_hidden\n",
        "        self.attn_in = (dim_en_hidden * 2) + dim_de_hidden\n",
        "        self.attn = nn.Linear(self.attn_in, dim_attention)\n",
        "\n",
        "    def forward(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)))\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-UsGaDndPOA"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim_output: int, dim_de_embedding: int, dim_en_hidden: int, dim_de_hidden: int, dropout: float, attention: nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_de_embedding = dim_de_embedding\n",
        "        self.dim_en_hidden = dim_en_hidden\n",
        "        self.dim_de_hidden = dim_de_hidden\n",
        "        self.dim_output = dim_output\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(dim_output, dim_de_embedding)\n",
        "        self.rnn = nn.GRU((dim_en_hidden * 2) + dim_de_embedding, dim_de_hidden)\n",
        "        self.out = nn.Linear(self.attention.attn_in + dim_de_embedding, dim_output)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _weighted_encoder_rep(self, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tensor:\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "    def forward(self, input: Tensor, decoder_hidden: Tensor, encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim=1))\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr97gAYKdyD_"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: nn.Module, decoder: nn.Module, device: torch.device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.dim_output\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <bos> token\n",
        "        output = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci-uvEvYeWfg"
      },
      "source": [
        "data = DataPipeline(batch_size=16)\n",
        "en_vocab = data.en_vocab\n",
        "de_vocab = data.de_vocab\n",
        "train_loader = data.train_loader\n",
        "valid_loader = data.valid_loader\n",
        "test_loader = data.test_loader\n",
        "\n",
        "dim_input = len(en_vocab)\n",
        "dim_output = len(de_vocab)\n",
        "dim_en_embedding = en_vocab.vectors.size()[1]\n",
        "dim_de_embedding = 64\n",
        "dim_en_hidden = 128\n",
        "dim_de_hidden = 128\n",
        "dim_attention = 32\n",
        "dropout_en = 0.5\n",
        "dropout_de = 0.5\n",
        "\n",
        "enc = Encoder(dim_input, dim_en_embedding, dim_en_hidden, dim_de_hidden, dropout_en)\n",
        "attn = Attention(dim_en_hidden, dim_de_hidden, dim_attention)\n",
        "dec = Decoder(dim_output, dim_de_embedding, dim_en_hidden, dim_de_hidden, dropout_de, attn)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab.stoi['<pad>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ1-9D_7hVrO"
      },
      "source": [
        "def train(model: nn.Module, iterator: torch.utils.data.DataLoader, optimizer: optim.Optimizer, criterion: nn.Module, clip: float):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QEnGcRNha7P"
      },
      "source": [
        "def evaluate(model: nn.Module, iterator: torch.utils.data.DataLoader, criterion: nn.Module):\n",
        "    model.eval()\n",
        "    epoch_loss, epoch_bleu = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for _, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "            epoch_bleu += bleu(output[1:, :, :], trg[1:, :], de_vocab, device)\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator), epoch_bleu / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcMSPA_DUN9L"
      },
      "source": [
        "def calculate_time(start_time: float, end_time: float):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def bleu(output, target, vocab, device):\n",
        "    _, output_indices = torch.max(output, 2)\n",
        "    eos_pad = torch.full((1, output.size()[1]), vocab.stoi[\"<eos>\"], device=device)\n",
        "    output_indices = torch.cat((output_indices, eos_pad), 0)\n",
        "    output_indices = output_indices.transpose(1, 0)\n",
        "    target = target.transpose(1, 0)\n",
        "\n",
        "    output_str = [[vocab.itos[x] for x in y] for y in output_indices]\n",
        "    output_str = [x[:x.index(\"<eos>\")] for x in output_str]\n",
        "    target_str = [[vocab.itos[x] for x in y] for y in target]\n",
        "    target_str = [[x[:x.index(\"<eos>\")]] for x in target_str]\n",
        "    return bleu_score(output_str, target_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DceAAMunhfQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618779cc-7daa-4585-8350-8cb0a65d1a65"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "\n",
        "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss, valid_bleu = evaluate(model, valid_loader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = calculate_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\tVal. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val. Bleu: {round(100*valid_bleu, 2)}')\n",
        "\n",
        "test_loss, test_bleu = evaluate(model, test_loader, criterion)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 15,110,013 trainable parameters\n",
            "Epoch: 01 | Time: 2m 10s\n",
            "\tTrain Loss: 7.251 | Train PPL: 1409.779\n",
            "\tVal. Loss: 7.599 |  Val. PPL: 1996.950 | Val. Bleu: 0.0\n",
            "Epoch: 02 | Time: 2m 12s\n",
            "\tTrain Loss: 6.629 | Train PPL: 756.478\n",
            "\tVal. Loss: 7.623 |  Val. PPL: 2044.602 | Val. Bleu: 0.05\n",
            "Epoch: 03 | Time: 2m 13s\n",
            "\tTrain Loss: 6.144 | Train PPL: 465.779\n",
            "\tVal. Loss: 7.713 |  Val. PPL: 2236.797 | Val. Bleu: 0.07\n",
            "Epoch: 04 | Time: 2m 12s\n",
            "\tTrain Loss: 5.706 | Train PPL: 300.748\n",
            "\tVal. Loss: 7.881 |  Val. PPL: 2647.570 | Val. Bleu: 0.06\n",
            "Epoch: 05 | Time: 2m 12s\n",
            "\tTrain Loss: 5.299 | Train PPL: 200.039\n",
            "\tVal. Loss: 7.982 |  Val. PPL: 2926.870 | Val. Bleu: 0.1\n",
            "Epoch: 06 | Time: 2m 12s\n",
            "\tTrain Loss: 4.945 | Train PPL: 140.478\n",
            "\tVal. Loss: 8.123 |  Val. PPL: 3372.609 | Val. Bleu: 0.23\n",
            "Epoch: 07 | Time: 2m 13s\n",
            "\tTrain Loss: 4.649 | Train PPL: 104.526\n",
            "\tVal. Loss: 8.287 |  Val. PPL: 3970.586 | Val. Bleu: 0.19\n",
            "Epoch: 08 | Time: 2m 12s\n",
            "\tTrain Loss: 4.410 | Train PPL:  82.256\n",
            "\tVal. Loss: 8.433 |  Val. PPL: 4596.434 | Val. Bleu: 0.19\n",
            "Epoch: 09 | Time: 2m 13s\n",
            "\tTrain Loss: 4.218 | Train PPL:  67.876\n",
            "\tVal. Loss: 8.523 |  Val. PPL: 5026.748 | Val. Bleu: 0.31\n",
            "Epoch: 10 | Time: 2m 12s\n",
            "\tTrain Loss: 4.079 | Train PPL:  59.069\n",
            "\tVal. Loss: 8.609 |  Val. PPL: 5478.959 | Val. Bleu: 0.3\n",
            "Epoch: 11 | Time: 2m 13s\n",
            "\tTrain Loss: 3.960 | Train PPL:  52.461\n",
            "\tVal. Loss: 8.692 |  Val. PPL: 5955.270 | Val. Bleu: 0.35\n",
            "Epoch: 12 | Time: 2m 13s\n",
            "\tTrain Loss: 3.851 | Train PPL:  47.026\n",
            "\tVal. Loss: 8.773 |  Val. PPL: 6460.577 | Val. Bleu: 0.37\n",
            "Epoch: 13 | Time: 2m 12s\n",
            "\tTrain Loss: 3.757 | Train PPL:  42.807\n",
            "\tVal. Loss: 8.847 |  Val. PPL: 6953.203 | Val. Bleu: 0.37\n",
            "Epoch: 14 | Time: 2m 13s\n",
            "\tTrain Loss: 3.674 | Train PPL:  39.423\n",
            "\tVal. Loss: 8.927 |  Val. PPL: 7535.376 | Val. Bleu: 0.4\n",
            "Epoch: 15 | Time: 2m 12s\n",
            "\tTrain Loss: 3.607 | Train PPL:  36.841\n",
            "\tVal. Loss: 8.942 |  Val. PPL: 7648.681 | Val. Bleu: 0.4\n",
            "Epoch: 16 | Time: 2m 12s\n",
            "\tTrain Loss: 3.534 | Train PPL:  34.245\n",
            "\tVal. Loss: 8.985 |  Val. PPL: 7984.730 | Val. Bleu: 0.38\n",
            "Epoch: 17 | Time: 2m 12s\n",
            "\tTrain Loss: 3.453 | Train PPL:  31.598\n",
            "\tVal. Loss: 9.090 |  Val. PPL: 8868.326 | Val. Bleu: 0.5\n",
            "Epoch: 18 | Time: 2m 13s\n",
            "\tTrain Loss: 3.413 | Train PPL:  30.361\n",
            "\tVal. Loss: 9.102 |  Val. PPL: 8971.793 | Val. Bleu: 0.41\n",
            "Epoch: 19 | Time: 2m 12s\n",
            "\tTrain Loss: 3.352 | Train PPL:  28.565\n",
            "\tVal. Loss: 9.158 |  Val. PPL: 9491.161 | Val. Bleu: 0.54\n",
            "Epoch: 20 | Time: 2m 13s\n",
            "\tTrain Loss: 3.291 | Train PPL:  26.876\n",
            "\tVal. Loss: 9.215 |  Val. PPL: 10043.029 | Val. Bleu: 0.52\n",
            "\tTest Loss: 9.215 | Test PPL: 10042.775 | Test Bleu: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrztZO9sZmAU",
        "outputId": "7e83de0f-bf88-43b0-f843-cc51b8796af7"
      },
      "source": [
        "print(f'\\tTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test Bleu: {round(100*test_bleu, 2)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTest Loss: 9.215 | Test PPL: 10042.775 | Test Bleu: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xPGN1iKhkM2"
      },
      "source": [
        "torch.save(model, '/content/drive/MyDrive/AI/project3/model1.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}